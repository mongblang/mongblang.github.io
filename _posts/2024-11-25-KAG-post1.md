---
title: "캐글 필사 - CTR Prediction"
date: 2024-11-22 
categories: [Kaggle]
tags: [Kaggle, 데이터분석]
layout: post
author: mongblang
---

[Competition Link](https://www.kaggle.com/competitions/avazu-ctr-prediction/overview)  
[Code Link](https://www.kaggle.com/code/akishen74/ctr-practice)  


### 1. **라이브러리 불러오기**

```python
# import library

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# import data

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# import seaborn

from matplotlib import pyplot as plt
import seaborn as sns
import gzip

test_file = '../input/avazu-ctr-prediction/test.gz'
samplesubmision_file = '../input/avazu-ctr-prediction/sampleSubmission.gz'
```

---

### 2. **데이터 불러오기**  
**파일의 Row 수가 너무 많기 때문에(4천만 건 이상) 데이터를 100만 건당 배치로 읽히고, 각 배치의 데이터에서 5%의 데이터를 무작위로 추출해 새로운 train set을 통합한다.**  

```python  
chunksize = 10 ** 6  
num_of_chunk = 0  
train = pd.DataFrame()  
for chunk in pd.read_csv('../input/avazu-ctr-train/train.csv', chunksize=chunksize):  
    num_of_chunk += 1       
    train = pd.concat([train, chunk.sample(frac=.05, replace=False, random_state=123)], axis=0) 
    print('Processing Chunk No. ' + str(num_of_chunk)) 
    
train.reset_index(inplace=True)  

train_len = len(train) 
train_len
```  
##### 세부 과정  
1. 한 번에 읽어올 데이터의 크기 (청크)를 100만개로 설정  
2. 현재까지 처리한 chunk의 개수를 기록  
3. 이후 청크에서 추출한 데이터를 모두 합쳐 저장할 빈 데이터프레임 준비  
4. 반복문을 사용해 청크에서 5%행만 랜덤으로 추출해 합치고 현재 몇 번째 청크를 처리중인지 표시해 진행 상태를 확인   
    - replace=False : 샘플링 시 행을 중복으로 선택하지 않도록 설정   
5. 데이터가 여러 청크에서 이어붙여지면서 인덱스가 꼬일 수 있으므로 기존 인덱스 초기화 
6. 나중에 df에서 인덱스 용도를 다시 분할할 수 있도록 train len을 백업

&nbsp;  

### 3. 전처리
**train과 test를 하나의 df로 통합해 자료 전처리를 병행할 수 있도록 한다.**  
```python
df = pd.concat([train, pd.read_csv(test_file, compression='gzip')]).drop(['index', 'id'], axis=1) 
df.head()
```
6. gzip 형식으로 저장된 test 파일을 판다스가 압축을 풀면서 읽고 test데이터를 train과 합친 후 인덱스, id열 삭제
7. 데이터프레임 샘플 확인 

